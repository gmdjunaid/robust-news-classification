{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust News Classification: Main Experiments\n",
    "\n",
    "This notebook ties together all components of the robust news classification project, implementing the complete experimental pipeline as specified in the project proposal.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Data Loading & Preprocessing**: Loading ISOT dataset and applying text cleaning\n",
    "2. **Data Splitting**: Both random splits and topic-holdout splits for robustness evaluation\n",
    "3. **Baseline Models**: TF-IDF + Logistic Regression and TF-IDF + Linear SVM\n",
    "4. **Advanced Models**: Sentence-embedding models and transformer-based classifiers\n",
    "5. **Evaluation**: Comprehensive metrics (Macro-F1, PR-AUC, ROC-AUC) under different split strategies\n",
    "6. **Cross-Dataset Transfer**: Zero-shot evaluation on external datasets (WELFake)\n",
    "\n",
    "## Project Goals\n",
    "\n",
    "- Evaluate model robustness under topic shifts (topic-holdout splits)\n",
    "- Compare interpretable baselines with advanced embedding/transformer models\n",
    "- Test cross-dataset generalization to assess real-world applicability\n",
    "- Use Macro-F1 as primary metric to balance class performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import all necessary modules from the `src/` directory and configure settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path for imports\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "# Import using importlib for files with numeric prefixes\n",
    "import importlib.util\n",
    "\n",
    "# Import preprocessing utilities\n",
    "spec = importlib.util.spec_from_file_location(\"preprocessing\", project_root / \"src\" / \"01_preprocessing.py\")\n",
    "preprocessing = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(preprocessing)\n",
    "from preprocessing import load_isot, apply_cleaning, clean_text\n",
    "\n",
    "# Import data splitting\n",
    "spec = importlib.util.spec_from_file_location(\"data_splitting\", project_root / \"src\" / \"02_data_splitting.py\")\n",
    "data_splitting = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(data_splitting)\n",
    "from data_splitting import random_split, topic_holdout_split\n",
    "\n",
    "# Import baseline models\n",
    "spec = importlib.util.spec_from_file_location(\"baseline_models\", project_root / \"src\" / \"03_baseline_models.py\")\n",
    "baseline_models = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(baseline_models)\n",
    "from baseline_models import build_tfidf, train_logreg, train_svm\n",
    "\n",
    "# Import evaluation\n",
    "spec = importlib.util.spec_from_file_location(\"baseline_eval\", project_root / \"src\" / \"04_baseline_eval.py\")\n",
    "baseline_eval = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(baseline_eval)\n",
    "from baseline_eval import evaluate\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "Load the ISOT dataset (Fake and True news) and apply text cleaning to remove noise and standardize formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ISOT dataset\n",
    "print(\"Loading ISOT dataset...\")\n",
    "df = load_isot(\n",
    "    fake_path='../data/training/Fake.csv',\n",
    "    real_path='../data/training/True.csv'\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nSubject distribution:\")\n",
    "print(df['subject'].value_counts())\n",
    "\n",
    "# Apply text cleaning\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Applying text cleaning...\")\n",
    "df = apply_cleaning(df, text_column='text')\n",
    "df = apply_cleaning(df, text_column='title')\n",
    "\n",
    "print(f\"\\nSample cleaned text (first 200 chars):\")\n",
    "print(df['text_cleaned'].iloc[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Splitting Strategies\n",
    "\n",
    "Create both random and topic-holdout splits to evaluate model robustness under different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split (for baseline comparison)\n",
    "print(\"=\"*60)\n",
    "print(\"RANDOM SPLIT\")\n",
    "print(\"=\"*60)\n",
    "df_train_random, df_test_random = random_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Topic-holdout split (for robustness evaluation)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOPIC HOLDOUT SPLIT\")\n",
    "print(\"=\"*60)\n",
    "# Hold out 'politicsNews' topic (typically the largest category)\n",
    "df_train_topic, df_test_topic = topic_holdout_split(\n",
    "    df, \n",
    "    topic_column='subject',\n",
    "    heldout_topic='politicsNews'\n",
    ")\n",
    "\n",
    "# Prepare text and labels for both splits\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Preparing data for modeling...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Random split\n",
    "X_train_random = df_train_random['text_cleaned'].tolist()\n",
    "X_test_random = df_test_random['text_cleaned'].tolist()\n",
    "y_train_random = df_train_random['label'].values\n",
    "y_test_random = df_test_random['label'].values\n",
    "\n",
    "# Topic-holdout split\n",
    "X_train_topic = df_train_topic['text_cleaned'].tolist()\n",
    "X_test_topic = df_test_topic['text_cleaned'].tolist()\n",
    "y_train_topic = df_train_topic['label'].values\n",
    "y_test_topic = df_test_topic['label'].values\n",
    "\n",
    "print(f\"Random split - Train: {len(X_train_random)}, Test: {len(X_test_random)}\")\n",
    "print(f\"Topic split - Train: {len(X_train_topic)}, Test: {len(X_test_topic)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Models: TF-IDF + Linear Classifiers\n",
    "\n",
    "Train interpretable baseline models using TF-IDF features with Logistic Regression and Linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TF-IDF vectorizer\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE MODELS: TF-IDF + Linear Classifiers\")\n",
    "print(\"=\"*60)\n",
    "vectorizer = build_tfidf(max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "# Transform text data\n",
    "X_train_random_tfidf = vectorizer.fit_transform(X_train_random)\n",
    "X_test_random_tfidf = vectorizer.transform(X_test_random)\n",
    "X_train_topic_tfidf = vectorizer.fit_transform(X_train_topic)\n",
    "X_test_topic_tfidf = vectorizer.transform(X_test_topic)\n",
    "\n",
    "print(f\"\\nTF-IDF feature matrix shape: {X_train_random_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate Logistic Regression on random split\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Logistic Regression - Random Split\")\n",
    "print(\"=\"*60)\n",
    "model_lr_random = train_logreg(X_train_random_tfidf, y_train_random)\n",
    "results_lr_random = evaluate(model_lr_random, X_test_random_tfidf, y_test_random, \n",
    "                             model_name=\"Logistic Regression (Random Split)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate Logistic Regression on topic-holdout split\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Logistic Regression - Topic Holdout Split\")\n",
    "print(\"=\"*60)\n",
    "model_lr_topic = train_logreg(X_train_topic_tfidf, y_train_topic)\n",
    "results_lr_topic = evaluate(model_lr_topic, X_test_topic_tfidf, y_test_topic,\n",
    "                            model_name=\"Logistic Regression (Topic Holdout)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate Linear SVM on random split\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Linear SVM - Random Split\")\n",
    "print(\"=\"*60)\n",
    "model_svm_random = train_svm(X_train_random_tfidf, y_train_random)\n",
    "results_svm_random = evaluate(model_svm_random, X_test_random_tfidf, y_test_random,\n",
    "                              model_name=\"Linear SVM (Random Split)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate Linear SVM on topic-holdout split\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Linear SVM - Topic Holdout Split\")\n",
    "print(\"=\"*60)\n",
    "model_svm_topic = train_svm(X_train_topic_tfidf, y_train_topic)\n",
    "results_svm_topic = evaluate(model_svm_topic, X_test_topic_tfidf, y_test_topic,\n",
    "                             model_name=\"Linear SVM (Topic Holdout)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Models: Sentence Embeddings\n",
    "\n",
    "Train models using sentence embeddings as features, providing richer semantic representations than TF-IDF.\n",
    "\n",
    "**Note**: This section requires the `sentence-transformers` library. Uncomment and run if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import embedding utilities\n",
    "spec = importlib.util.spec_from_file_location(\"embeddings_model\", project_root / \"src\" / \"05_embeddings_model.py\")\n",
    "embeddings_model = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(embeddings_model)\n",
    "from embeddings_model import build_embeddings, embed_text, train_embedding_classifier\n",
    "\n",
    "# Build sentence embedding model\n",
    "print(\"=\"*60)\n",
    "print(\"ADVANCED MODELS: Sentence Embeddings\")\n",
    "print(\"=\"*60)\n",
    "embedder = build_embeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Compute embeddings for random split\n",
    "print(\"\\nComputing embeddings for random split...\")\n",
    "emb_train_random = embed_text(embedder, X_train_random)\n",
    "emb_test_random = embed_text(embedder, X_test_random)\n",
    "\n",
    "# Compute embeddings for topic-holdout split\n",
    "print(\"Computing embeddings for topic-holdout split...\")\n",
    "emb_train_topic = embed_text(embedder, X_train_topic)\n",
    "emb_test_topic = embed_text(embedder, X_test_topic)\n",
    "\n",
    "print(f\"Embedding dimension: {emb_train_random.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate embedding-based classifier on random split\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Embedding-based Classifier - Random Split\")\n",
    "print(\"=\"*60)\n",
    "model_emb_random = train_embedding_classifier(emb_train_random, y_train_random)\n",
    "results_emb_random = evaluate(model_emb_random, emb_test_random, y_test_random,\n",
    "                              model_name=\"Embedding Classifier (Random Split)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate embedding-based classifier on topic-holdout split\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Embedding-based Classifier - Topic Holdout Split\")\n",
    "print(\"=\"*60)\n",
    "model_emb_topic = train_embedding_classifier(emb_train_topic, y_train_topic)\n",
    "results_emb_topic = evaluate(model_emb_topic, emb_test_topic, y_test_topic,\n",
    "                             model_name=\"Embedding Classifier (Topic Holdout)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Summary\n",
    "\n",
    "Compare all models across both split strategies to assess robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results summary\n",
    "print(\"=\"*60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_summary = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Logistic Regression (TF-IDF)',\n",
    "        'Linear SVM (TF-IDF)',\n",
    "        'Embedding Classifier',\n",
    "    ],\n",
    "    'Random Split - Macro-F1': [\n",
    "        results_lr_random['f1_macro'],\n",
    "        results_svm_random['f1_macro'],\n",
    "        results_emb_random['f1_macro'] if 'results_emb_random' in locals() else None,\n",
    "    ],\n",
    "    'Random Split - ROC-AUC': [\n",
    "        results_lr_random['roc_auc'],\n",
    "        results_svm_random['roc_auc'],\n",
    "        results_emb_random['roc_auc'] if 'results_emb_random' in locals() else None,\n",
    "    ],\n",
    "    'Topic Holdout - Macro-F1': [\n",
    "        results_lr_topic['f1_macro'],\n",
    "        results_svm_topic['f1_macro'],\n",
    "        results_emb_topic['f1_macro'] if 'results_emb_topic' in locals() else None,\n",
    "    ],\n",
    "    'Topic Holdout - ROC-AUC': [\n",
    "        results_lr_topic['roc_auc'],\n",
    "        results_svm_topic['roc_auc'],\n",
    "        results_emb_topic['roc_auc'] if 'results_emb_topic' in locals() else None,\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(results_summary.to_string(index=False))\n",
    "\n",
    "# Calculate robustness gap (performance drop from random to topic-holdout)\n",
    "results_summary['Macro-F1 Drop'] = (\n",
    "    results_summary['Random Split - Macro-F1'] - \n",
    "    results_summary['Topic Holdout - Macro-F1']\n",
    ")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Robustness Analysis (Macro-F1 drop from random to topic-holdout):\")\n",
    "print(\"=\"*60)\n",
    "print(results_summary[['Model', 'Macro-F1 Drop']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Dataset Transfer Evaluation\n",
    "\n",
    "Evaluate model generalization by testing on external datasets (WELFake) without fine-tuning.\n",
    "\n",
    "**Note**: This section requires the WELFake dataset. Adjust paths as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cross-dataset utilities\n",
    "spec = importlib.util.spec_from_file_location(\"cross_dataset_transfer\", project_root / \"src\" / \"07_cross_dataset_transfer.py\")\n",
    "cross_dataset_transfer = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(cross_dataset_transfer)\n",
    "from cross_dataset_transfer import load_kaggle_dataset, zero_shot_test\n",
    "\n",
    "# Load WELFake dataset for cross-dataset evaluation\n",
    "print(\"=\"*60)\n",
    "print(\"CROSS-DATASET TRANSFER: WELFake Dataset\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    df_welfake = load_kaggle_dataset(\n",
    "        path='../data/test/WELFake_Dataset_sample_1000.csv',\n",
    "        text_column='text',\n",
    "        label_column='label',\n",
    "        text_cleaner=clean_text\n",
    "    )\n",
    "\n",
    "    print(f\"WELFake dataset shape: {df_welfake.shape}\")\n",
    "    print(f\"Label distribution:\")\n",
    "    print(df_welfake['label'].value_counts())\n",
    "\n",
    "    X_welfake = df_welfake['text_cleaned'].tolist()\n",
    "    y_welfake = df_welfake['label'].values\n",
    "\n",
    "    # Evaluate baseline models on WELFake (zero-shot)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Zero-Shot Evaluation on WELFake Dataset\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Transform WELFake text with TF-IDF vectorizer (trained on ISOT)\n",
    "    X_welfake_tfidf = vectorizer.transform(X_welfake)\n",
    "\n",
    "    print(\"\\nLogistic Regression (TF-IDF) - WELFake:\")\n",
    "    f1_lr_welfake = zero_shot_test(model_lr_random, X_welfake_tfidf, y_welfake)\n",
    "    print(f\"Macro-F1: {f1_lr_welfake:.4f}\")\n",
    "\n",
    "    print(\"\\nLinear SVM (TF-IDF) - WELFake:\")\n",
    "    f1_svm_welfake = zero_shot_test(model_svm_random, X_welfake_tfidf, y_welfake)\n",
    "    print(f\"Macro-F1: {f1_svm_welfake:.4f}\")\n",
    "\n",
    "    if 'emb_welfake' in locals():\n",
    "        print(\"\\nEmbedding Classifier - WELFake:\")\n",
    "        emb_welfake = embed_text(embedder, X_welfake)\n",
    "        f1_emb_welfake = zero_shot_test(model_emb_random, emb_welfake, y_welfake)\n",
    "        print(f\"Macro-F1: {f1_emb_welfake:.4f}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"WELFake dataset not found: {e}\")\n",
    "    print(\"Skipping cross-dataset evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions and Discussion\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Model Performance**: Compare baseline (TF-IDF) vs. advanced (embeddings, transformers) approaches\n",
    "2. **Robustness**: Assess performance drop from random splits to topic-holdout splits\n",
    "3. **Generalization**: Evaluate cross-dataset transfer performance on WELFake\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- **Macro-F1** serves as the primary metric to balance performance across classes\n",
    "- **Topic-holdout splits** reveal whether models rely on topic-specific shortcuts\n",
    "- **Cross-dataset evaluation** tests real-world applicability beyond ISOT\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Fine-tune hyperparameters for optimal performance\n",
    "- Analyze failure cases and model interpretability\n",
    "- Expand cross-dataset evaluation to additional external datasets\n",
    "- Add transformer model evaluation (requires additional setup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
