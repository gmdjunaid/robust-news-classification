{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust News Classification: Main Experiments\n",
    "\n",
    "This notebook ties together all components of the robust news classification project for the current flow (train on full ISOT, test on held-out files).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Data Loading & Preprocessing**: Load ISOT training data and clean text\n",
    "2. **Training**: Train on the full ISOT training set (Fake + True)\n",
    "3. **Baseline Models**: TF-IDF + LogReg/SVM\n",
    "4. **Advanced Models**: Sentence embeddings (MiniLM)\n",
    "5. **Evaluation**:\n",
    "   - Fake-only test (`data/test/fake.csv`): false negatives / fake recall\n",
    "   - Mixed labeled test (`data/test/WELFake_Dataset_sample_10000.csv`): Macro-F1, ROC-AUC, PR-AUC, confusion matrix\n",
    "6. **Cross-Dataset Transfer**: WELFake evaluation covers the external test without fine-tuning\n",
    "\n",
    "## Project Goals\n",
    "\n",
    "- Train on full labeled ISOT data (text only)\n",
    "- Check false negatives on fake-only held-out data\n",
    "- Measure balanced metrics on a mixed external set (WELFake)\n",
    "- Compare TF-IDF baselines with embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import all necessary modules from the `src/` directory and configure settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;124m'\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps:\u001b[39m\u001b[38;5;124m'\u001b[39m, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available())\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('cuda:', torch.cuda.is_available())\n",
    "print('mps:', torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "Project root: /Users/reuben/robust-news-classification/robust-news-classification\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path for imports\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "# Import using importlib for files with numeric prefixes\n",
    "import importlib.util\n",
    "\n",
    "# Import preprocessing utilities\n",
    "spec = importlib.util.spec_from_file_location(\"preprocessing\", project_root / \"src\" / \"01_preprocessing.py\")\n",
    "preprocessing = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(preprocessing)\n",
    "sys.modules[\"preprocessing\"] = preprocessing\n",
    "from preprocessing import load_isot, apply_cleaning, clean_text\n",
    "\n",
    "# Import baseline models\n",
    "spec = importlib.util.spec_from_file_location(\"baseline_models\", project_root / \"src\" / \"03_baseline_models.py\")\n",
    "baseline_models = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(baseline_models)\n",
    "sys.modules[\"baseline_models\"] = baseline_models\n",
    "from baseline_models import build_tfidf, train_logreg, train_svm\n",
    "\n",
    "# Import evaluation\n",
    "spec = importlib.util.spec_from_file_location(\"baseline_eval\", project_root / \"src\" / \"04_baseline_eval.py\")\n",
    "baseline_eval = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(baseline_eval)\n",
    "sys.modules[\"baseline_eval\"] = baseline_eval\n",
    "from baseline_eval import evaluate\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "Load the ISOT dataset (Fake and True news) and apply text cleaning to remove noise and standardize formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ISOT dataset...\n",
      "Loading fake news from ../data/training/Fake.csv...\n",
      "Loading real news from ../data/training/True.csv...\n",
      "Loaded 23481 fake articles and 21417 real articles\n",
      "Total: 44898 articles\n",
      "\n",
      "Dataset shape: (44898, 6)\n",
      "Columns: ['title', 'text', 'subject', 'date', 'label', 'source_file']\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "1    23481\n",
      "0    21417\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Subject distribution:\n",
      "subject\n",
      "politicsNews       11272\n",
      "worldnews          10145\n",
      "News                9050\n",
      "politics            6841\n",
      "left-news           4459\n",
      "Government News     1570\n",
      "US_News              783\n",
      "Middle-east          778\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "Applying text cleaning...\n",
      "Applied text cleaning to column 'text'\n",
      "Created new column 'text_cleaned' with cleaned text\n",
      "Applied text cleaning to column 'title'\n",
      "Created new column 'title_cleaned' with cleaned text\n",
      "\n",
      "Sample cleaned text (first 200 chars):\n",
      "Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and the very dishonest fake news media. The former reali\n"
     ]
    }
   ],
   "source": [
    "# Load ISOT dataset\n",
    "print(\"Loading ISOT dataset...\")\n",
    "df = load_isot(\n",
    "    fake_path='../data/training/Fake.csv',\n",
    "    real_path='../data/training/True.csv'\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nSubject distribution:\")\n",
    "print(df['subject'].value_counts())\n",
    "\n",
    "# Apply text cleaning\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Applying text cleaning...\")\n",
    "df = apply_cleaning(df, text_column='text')\n",
    "df = apply_cleaning(df, text_column='title')\n",
    "\n",
    "print(f\"\\nSample cleaned text (first 200 chars):\")\n",
    "print(df['text_cleaned'].iloc[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Test Setup\n",
    "\n",
    "Train on the full ISOT training set (Fake + True). Evaluate on two held-out files:\n",
    "- `data/test/fake.csv` (all fake) to measure false negatives (fake recall)\n",
    "- `data/test/WELFake_Dataset_sample_10000.csv` (mixed, labeled) for full metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Preparing train and held-out test sets\n",
      "============================================================\n",
      "\n",
      "Loading fake-only test set (all fake)...\n",
      "\n",
      "Loading WELFake test set (mixed labeled)...\n",
      "Train size: 44898\n",
      "Fake-only test size: 12999\n",
      "WELFake test size: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Preparing train and held-out test sets\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train on full ISOT training set\n",
    "X_train_full = df['text_cleaned'].tolist()\n",
    "y_train_full = df['label'].values\n",
    "\n",
    "# Fake-only test set: measure false negatives / fake recall\n",
    "print(\"\\nLoading fake-only test set (all fake)...\")\n",
    "df_fake_test = pd.read_csv('../data/test/fake.csv')\n",
    "df_fake_test['text_cleaned'] = df_fake_test['text'].apply(clean_text)\n",
    "X_test_fake = df_fake_test['text_cleaned'].tolist()\n",
    "# Fake class = 1 (original convention)\n",
    "y_test_fake = np.ones(len(df_fake_test), dtype=int)\n",
    "\n",
    "# WELFake mixed test set: full metrics\n",
    "print(\"\\nLoading WELFake test set (mixed labeled)...\")\n",
    "df_welfake = pd.read_csv('../data/test/WELFake_Dataset_sample_10000.csv')\n",
    "# WELFake labels are actually 0=real, 1=fake; this matches our convention, so no mapping needed\n",
    "# If you find labels are reversed, flip with {0:1, 1:0}\n",
    "df_welfake['text_cleaned'] = df_welfake['text'].apply(clean_text)\n",
    "X_test_welfake = df_welfake['text_cleaned'].tolist()\n",
    "y_test_welfake = df_welfake['label'].values\n",
    "\n",
    "print(f\"Train size: {len(X_train_full)}\")\n",
    "print(f\"Fake-only test size: {len(X_test_fake)}\")\n",
    "print(f\"WELFake test size: {len(X_test_welfake)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Models: TF-IDF + Linear Classifiers\n",
    "\n",
    "Train interpretable baseline models using TF-IDF features with Logistic Regression and Linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASELINE MODELS: TF-IDF + Linear Classifiers\n",
      "============================================================\n",
      "Created TF-IDF vectorizer with:\n",
      "  max_features: 5000\n",
      "  min_df: 2\n",
      "  max_df: 0.95\n",
      "  ngram_range: (1, 2)\n",
      "  stop_words: english\n",
      "\n",
      "TF-IDF feature matrix shape: (44898, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Build TF-IDF vectorizer\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE MODELS: TF-IDF + Linear Classifiers\")\n",
    "print(\"=\"*60)\n",
    "vectorizer = build_tfidf(max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "# Transform text data (fit once on full training set)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_full)\n",
    "X_test_fake_tfidf = vectorizer.transform(X_test_fake)\n",
    "X_test_welfake_tfidf = vectorizer.transform(X_test_welfake)\n",
    "\n",
    "print(f\"\\nTF-IDF feature matrix shape: {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Logistic Regression - Full Train -> WELFake\n",
      "============================================================\n",
      "Training Logistic Regression classifier...\n",
      "  Training samples: 44898\n",
      "  C (regularization): 1.0\n",
      "  max_iter: 1000\n",
      "  solver: lbfgs\n",
      "Training completed.\n",
      "  Training accuracy: 0.9925\n",
      "\n",
      "============================================================\n",
      "Evaluating Logistic Regression (WELFake)\n",
      "============================================================\n",
      "Test set size: 10000 samples\n",
      "Class distribution: {np.int64(0): np.int64(4844), np.int64(1): np.int64(5156)}\n",
      "\n",
      "Metrics                   Value          \n",
      "----------------------------------------\n",
      "Accuracy                  0.8358\n",
      "Precision (macro)         0.8656\n",
      "Recall (macro)            0.8313\n",
      "F1-score (macro)          0.8309  <-- PRIMARY METRIC\n",
      "F1-score (weighted)       0.8318\n",
      "ROC-AUC                   0.9048  <-- Secondary metric\n",
      "PR-AUC (Avg Precision)    0.8781  <-- Secondary metric\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predicted Real  Predicted Fake \n",
      "Actual Real     3332            1512           \n",
      "Actual Fake     130             5026           \n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.96      0.69      0.80      4844\n",
      "        Fake       0.77      0.97      0.86      5156\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.87      0.83      0.83     10000\n",
      "weighted avg       0.86      0.84      0.83     10000\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Logistic Regression on full train -> WELFake\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Logistic Regression - Full Train -> WELFake\")\n",
    "print(\"=\"*60)\n",
    "model_lr = train_logreg(X_train_tfidf, y_train_full)\n",
    "results_lr_welfake = evaluate(model_lr, X_test_welfake_tfidf, y_test_welfake, \n",
    "                             model_name=\"Logistic Regression (WELFake)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Linear SVM - Full Train -> WELFake\n",
      "============================================================\n",
      "Training Linear SVM classifier...\n",
      "  Training samples: 44898\n",
      "  C (regularization): 1.0\n",
      "  max_iter: 1000\n",
      "  dual: False\n",
      "Training completed.\n",
      "  Training accuracy: 0.9990\n",
      "\n",
      "============================================================\n",
      "Evaluating Linear SVM (WELFake)\n",
      "============================================================\n",
      "Test set size: 10000 samples\n",
      "Class distribution: {np.int64(0): np.int64(4844), np.int64(1): np.int64(5156)}\n",
      "\n",
      "Metrics                   Value          \n",
      "----------------------------------------\n",
      "Accuracy                  0.8343\n",
      "Precision (macro)         0.8688\n",
      "Recall (macro)            0.8295\n",
      "F1-score (macro)          0.8288  <-- PRIMARY METRIC\n",
      "F1-score (weighted)       0.8297\n",
      "ROC-AUC                   0.9033  <-- Secondary metric\n",
      "PR-AUC (Avg Precision)    0.8803  <-- Secondary metric\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predicted Real  Predicted Fake \n",
      "Actual Real     3274            1570           \n",
      "Actual Fake     87              5069           \n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.97      0.68      0.80      4844\n",
      "        Fake       0.76      0.98      0.86      5156\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.87      0.83      0.83     10000\n",
      "weighted avg       0.87      0.83      0.83     10000\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Linear SVM on full train -> WELFake\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Linear SVM - Full Train -> WELFake\")\n",
    "print(\"=\"*60)\n",
    "model_svm = train_svm(X_train_tfidf, y_train_full)\n",
    "results_svm_welfake = evaluate(model_svm, X_test_welfake_tfidf, y_test_welfake,\n",
    "                              model_name=\"Linear SVM (WELFake)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Fake-only test set (all fake) - TF-IDF models\n",
      "============================================================\n",
      "LogReg (fake-only): total=12999, false_negatives=826, fake_recall=0.9365\n",
      "Linear SVM (fake-only): total=12999, false_negatives=685, fake_recall=0.9473\n"
     ]
    }
   ],
   "source": [
    "# Fake-only test: false negatives / fake recall (TF-IDF models)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Fake-only test set (all fake) - TF-IDF models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def fake_only_report(model, X_fake, model_name):\n",
    "    preds = model.predict(X_fake)\n",
    "    total = len(preds)\n",
    "    # Fake class = 1; false negatives are predictions of 0 (real)\n",
    "    fn = np.sum(preds == 0)\n",
    "    recall_fake = 1 - fn / total if total else float('nan')\n",
    "    print(f\"{model_name}: total={total}, false_negatives={fn}, fake_recall={recall_fake:.4f}\")\n",
    "    return {\"total\": int(total), \"false_negatives\": int(fn), \"fake_recall\": float(recall_fake)}\n",
    "\n",
    "fake_only_lr = fake_only_report(model_lr, X_test_fake_tfidf, \"LogReg (fake-only)\")\n",
    "fake_only_svm = fake_only_report(model_svm, X_test_fake_tfidf, \"Linear SVM (fake-only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Models: Sentence Embeddings\n",
    "\n",
    "Train models using sentence embeddings as features, providing richer semantic representations than TF-IDF.\n",
    "\n",
    "**Note**: This section requires the `sentence-transformers` library. Uncomment and run if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ADVANCED MODELS: Sentence Embeddings\n",
      "============================================================\n",
      "Loading sentence-embedding model: all-MiniLM-L6-v2\n",
      "Using device for embeddings: mps\n",
      "Embedding model loaded successfully.\n",
      "\n",
      "Computing embeddings for full train and test sets...\n",
      "Encoding 44898 texts into embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388a6ecae01841298f8db42cb2147c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (44898, 384)\n",
      "Encoding 12999 texts into embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ac183c42c94d90bad55b7e6abe01b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (12999, 384)\n",
      "Encoding 10000 texts into embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04514f78b29c4bf09fbf7a01e807e064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (10000, 384)\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Import embedding utilities\n",
    "spec = importlib.util.spec_from_file_location(\"embeddings_model\", project_root / \"src\" / \"05_embeddings_model.py\")\n",
    "embeddings_model = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(embeddings_model)\n",
    "sys.modules[\"embeddings_model\"] = embeddings_model\n",
    "from embeddings_model import build_embeddings, embed_text, train_embedding_classifier\n",
    "\n",
    "# Build sentence embedding model\n",
    "print(\"=\"*60)\n",
    "print(\"ADVANCED MODELS: Sentence Embeddings\")\n",
    "print(\"=\"*60)\n",
    "embedder = build_embeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Compute embeddings for full train and tests\n",
    "print(\"\\nComputing embeddings for full train and test sets...\")\n",
    "emb_train_full = embed_text(embedder, X_train_full)\n",
    "emb_test_fake_emb = embed_text(embedder, X_test_fake)\n",
    "emb_test_welfake_emb = embed_text(embedder, X_test_welfake)\n",
    "\n",
    "print(f\"Embedding dimension: {emb_train_full.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Embedding-based Classifier - Full Train -> WELFake\n",
      "============================================================\n",
      "Training Logistic Regression classifier on embeddings...\n",
      "  Training samples: 44898\n",
      "  Embedding dimension: 384\n",
      "  C (regularization): 1.0\n",
      "  max_iter: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n",
      "  Training accuracy on embeddings: 0.9617\n",
      "\n",
      "============================================================\n",
      "Evaluating Embedding Classifier (WELFake)\n",
      "============================================================\n",
      "Test set size: 10000 samples\n",
      "Class distribution: {np.int64(0): np.int64(4844), np.int64(1): np.int64(5156)}\n",
      "\n",
      "Metrics                   Value          \n",
      "----------------------------------------\n",
      "Accuracy                  0.8042\n",
      "Precision (macro)         0.8110\n",
      "Recall (macro)            0.8018\n",
      "F1-score (macro)          0.8021  <-- PRIMARY METRIC\n",
      "F1-score (weighted)       0.8027\n",
      "ROC-AUC                   0.8843  <-- Secondary metric\n",
      "PR-AUC (Avg Precision)    0.8752  <-- Secondary metric\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predicted Real  Predicted Fake \n",
      "Actual Real     3506            1338           \n",
      "Actual Fake     620             4536           \n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.85      0.72      0.78      4844\n",
      "        Fake       0.77      0.88      0.82      5156\n",
      "\n",
      "    accuracy                           0.80     10000\n",
      "   macro avg       0.81      0.80      0.80     10000\n",
      "weighted avg       0.81      0.80      0.80     10000\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate embedding-based classifier -> WELFake\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Embedding-based Classifier - Full Train -> WELFake\")\n",
    "print(\"=\"*60)\n",
    "model_emb = train_embedding_classifier(emb_train_full, y_train_full)\n",
    "results_emb_welfake = evaluate(model_emb, emb_test_welfake_emb, y_test_welfake,\n",
    "                              model_name=\"Embedding Classifier (WELFake)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Fake-only test set (all fake) - Embedding model\n",
      "============================================================\n",
      "Embedding model: total=12999, false_negatives=3235, fake_recall=0.7511\n"
     ]
    }
   ],
   "source": [
    "# Fake-only test: false negatives / fake recall (embeddings)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Fake-only test set (all fake) - Embedding model\")\n",
    "print(\"=\"*60)\n",
    "emb_preds_fake = model_emb.predict(emb_test_fake_emb)\n",
    "total_fake = len(emb_preds_fake)\n",
    "# Fake class = 1; false negatives are predictions of 0 (real)\n",
    "fn_fake = np.sum(emb_preds_fake == 0)\n",
    "recall_fake_emb = 1 - fn_fake / total_fake if total_fake else float('nan')\n",
    "print(f\"Embedding model: total={total_fake}, false_negatives={fn_fake}, fake_recall={recall_fake_emb:.4f}\")\n",
    "fake_only_emb = {\"total\": int(total_fake), \"false_negatives\": int(fn_fake), \"fake_recall\": float(recall_fake_emb)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Summary\n",
    "\n",
    "Compare models on WELFake (mixed labeled) and report fake-only recall on `data/test/fake.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Dataset Transfer Evaluation\n",
    "\n",
    "Handled above by running the ISOT-trained models on WELFake (zero-shot, no fine-tuning). No additional code needed here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Reference) Cross-dataset transfer summary: see WELFake results above; no further actions here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions and Discussion\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Model Performance**: Compare baseline (TF-IDF) vs. advanced (embeddings) approaches\n",
    "2. **Generalization**: Evaluate cross-dataset transfer performance on WELFake\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- **Macro-F1** serves as the primary metric to balance performance across classes\n",
    "- **Topic-holdout splits** reveal whether models rely on topic-specific shortcuts\n",
    "- **Cross-dataset evaluation** tests real-world applicability beyond ISOT\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Fine-tune hyperparameters for optimal performance\n",
    "- Analyze failure cases and model interpretability\n",
    "- Expand cross-dataset evaluation to additional external datasets\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
