{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Robust News Classification: Main Experiments\n",
        "\n",
        "This notebook ties together all components of the robust news classification project for the current flow (train on full ISOT, test on held-out files).\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. **Data Loading & Preprocessing**: Load ISOT training data and clean text\n",
        "2. **Training**: Train on the full ISOT training set (Fake + True)\n",
        "3. **Baseline & Advanced Models**: TF-IDF + LogReg/SVM; optional sentence-embedding classifier\n",
        "4. **Evaluation**:\n",
        "   - Fake-only test (`data/test/fake.csv`): false negatives / fake recall\n",
        "   - Mixed labeled test (`data/test/WELFake_Dataset_sample_1000.csv`): Macro-F1, ROC-AUC, PR-AUC, confusion matrix\n",
        "5. **Cross-Dataset Transfer**: WELFake evaluation covers the external test without fine-tuning\n",
        "\n",
        "## Project Goals\n",
        "\n",
        "- Train on full labeled ISOT data (text only)\n",
        "- Check false negatives on fake-only held-out data\n",
        "- Measure balanced metrics on a mixed external set (WELFake)\n",
        "- Compare TF-IDF baselines with optional embedding model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n",
        "\n",
        "Import all necessary modules from the `src/` directory and configure settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src directory to path for imports\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root / 'src'))\n",
        "\n",
        "# Import using importlib for files with numeric prefixes\n",
        "import importlib.util\n",
        "\n",
        "# Import preprocessing utilities\n",
        "spec = importlib.util.spec_from_file_location(\"preprocessing\", project_root / \"src\" / \"01_preprocessing.py\")\n",
        "preprocessing = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(preprocessing)\n",
        "sys.modules[\"preprocessing\"] = preprocessing\n",
        "from preprocessing import load_isot, apply_cleaning, clean_text\n",
        "\n",
        "# Import baseline models\n",
        "spec = importlib.util.spec_from_file_location(\"baseline_models\", project_root / \"src\" / \"03_baseline_models.py\")\n",
        "baseline_models = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(baseline_models)\n",
        "sys.modules[\"baseline_models\"] = baseline_models\n",
        "from baseline_models import build_tfidf, train_logreg, train_svm\n",
        "\n",
        "# Import evaluation\n",
        "spec = importlib.util.spec_from_file_location(\"baseline_eval\", project_root / \"src\" / \"04_baseline_eval.py\")\n",
        "baseline_eval = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(baseline_eval)\n",
        "sys.modules[\"baseline_eval\"] = baseline_eval\n",
        "from baseline_eval import evaluate\n",
        "\n",
        "print(\"All imports successful!\")\n",
        "print(f\"Project root: {project_root}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Preprocessing\n",
        "\n",
        "Load the ISOT dataset (Fake and True news) and apply text cleaning to remove noise and standardize formatting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load ISOT dataset\n",
        "print(\"Loading ISOT dataset...\")\n",
        "df = load_isot(\n",
        "    fake_path='../data/training/Fake.csv',\n",
        "    real_path='../data/training/True.csv'\n",
        ")\n",
        "\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "print(f\"\\nSubject distribution:\")\n",
        "print(df['subject'].value_counts())\n",
        "\n",
        "# Apply text cleaning\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Applying text cleaning...\")\n",
        "df = apply_cleaning(df, text_column='text')\n",
        "df = apply_cleaning(df, text_column='title')\n",
        "\n",
        "print(f\"\\nSample cleaned text (first 200 chars):\")\n",
        "print(df['text_cleaned'].iloc[0][:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train/Test Setup\n",
        "\n",
        "Train on the full ISOT training set (Fake + True). Evaluate on two held-out files:\n",
        "- `data/test/fake.csv` (all fake) to measure false negatives (fake recall)\n",
        "- `data/test/WELFake_Dataset_sample_1000.csv` (mixed, labeled) for full metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"Preparing train and held-out test sets\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Train on full ISOT training set\n",
        "X_train_full = df['text_cleaned'].tolist()\n",
        "y_train_full = df['label'].values\n",
        "\n",
        "# Fake-only test set: measure false negatives / fake recall\n",
        "print(\"\\nLoading fake-only test set (all fake)...\")\n",
        "df_fake_test = pd.read_csv('../data/test/fake.csv')\n",
        "df_fake_test['text_cleaned'] = df_fake_test['text'].apply(clean_text)\n",
        "X_test_fake = df_fake_test['text_cleaned'].tolist()\n",
        "y_test_fake = np.ones(len(df_fake_test), dtype=int)\n",
        "\n",
        "# WELFake mixed test set: full metrics\n",
        "print(\"\\nLoading WELFake test set (mixed labeled)...\")\n",
        "df_welfake = pd.read_csv('../data/test/WELFake_Dataset_sample_1000.csv')\n",
        "# Map WELFake labels (source: 0=fake, 1=real) into project convention (1=fake, 0=real)\n",
        "df_welfake['label'] = df_welfake['label'].map({0: 1, 1: 0})\n",
        "df_welfake['text_cleaned'] = df_welfake['text'].apply(clean_text)\n",
        "X_test_welfake = df_welfake['text_cleaned'].tolist()\n",
        "y_test_welfake = df_welfake['label'].values\n",
        "\n",
        "print(f\"Train size: {len(X_train_full)}\")\n",
        "print(f\"Fake-only test size: {len(X_test_fake)}\")\n",
        "print(f\"WELFake test size: {len(X_test_welfake)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Baseline Models: TF-IDF + Linear Classifiers\n",
        "\n",
        "Train interpretable baseline models using TF-IDF features with Logistic Regression and Linear SVM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build TF-IDF vectorizer\n",
        "print(\"=\"*60)\n",
        "print(\"BASELINE MODELS: TF-IDF + Linear Classifiers\")\n",
        "print(\"=\"*60)\n",
        "vectorizer = build_tfidf(max_features=5000, ngram_range=(1, 2))\n",
        "\n",
        "# Transform text data (fit once on full training set)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train_full)\n",
        "X_test_fake_tfidf = vectorizer.transform(X_test_fake)\n",
        "X_test_welfake_tfidf = vectorizer.transform(X_test_welfake)\n",
        "\n",
        "print(f\"\\nTF-IDF feature matrix shape: {X_train_tfidf.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate Logistic Regression on full train -> WELFake\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Logistic Regression - Full Train -> WELFake\")\n",
        "print(\"=\"*60)\n",
        "model_lr = train_logreg(X_train_tfidf, y_train_full)\n",
        "results_lr_welfake = evaluate(model_lr, X_test_welfake_tfidf, y_test_welfake, \n",
        "                             model_name=\"Logistic Regression (WELFake)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate Linear SVM on full train -> WELFake\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Linear SVM - Full Train -> WELFake\")\n",
        "print(\"=\"*60)\n",
        "model_svm = train_svm(X_train_tfidf, y_train_full)\n",
        "results_svm_welfake = evaluate(model_svm, X_test_welfake_tfidf, y_test_welfake,\n",
        "                              model_name=\"Linear SVM (WELFake)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fake-only test: false negatives / fake recall (TF-IDF models)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Fake-only test set (all fake) - TF-IDF models\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def fake_only_report(model, X_fake, model_name):\n",
        "    preds = model.predict(X_fake)\n",
        "    total = len(preds)\n",
        "    fn = np.sum(preds == 0)\n",
        "    recall_fake = 1 - fn / total if total else float('nan')\n",
        "    print(f\"{model_name}: total={total}, false_negatives={fn}, fake_recall={recall_fake:.4f}\")\n",
        "    return {\"total\": int(total), \"false_negatives\": int(fn), \"fake_recall\": float(recall_fake)}\n",
        "\n",
        "fake_only_lr = fake_only_report(model_lr, X_test_fake_tfidf, \"LogReg (fake-only)\")\n",
        "fake_only_svm = fake_only_report(model_svm, X_test_fake_tfidf, \"Linear SVM (fake-only)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: Placeholder removed after refactor; no additional code needed here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Advanced Models: Sentence Embeddings\n",
        "\n",
        "Train models using sentence embeddings as features, providing richer semantic representations than TF-IDF.\n",
        "\n",
        "**Note**: This section requires the `sentence-transformers` library. Uncomment and run if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import embedding utilities\n",
        "spec = importlib.util.spec_from_file_location(\"embeddings_model\", project_root / \"src\" / \"05_embeddings_model.py\")\n",
        "embeddings_model = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(embeddings_model)\n",
        "sys.modules[\"embeddings_model\"] = embeddings_model\n",
        "from embeddings_model import build_embeddings, embed_text, train_embedding_classifier\n",
        "\n",
        "# Build sentence embedding model\n",
        "print(\"=\"*60)\n",
        "print(\"ADVANCED MODELS: Sentence Embeddings\")\n",
        "print(\"=\"*60)\n",
        "embedder = build_embeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Compute embeddings for full train and tests\n",
        "print(\"\\nComputing embeddings for full train and test sets...\")\n",
        "emb_train_full = embed_text(embedder, X_train_full)\n",
        "emb_test_fake_emb = embed_text(embedder, X_test_fake)\n",
        "emb_test_welfake_emb = embed_text(embedder, X_test_welfake)\n",
        "\n",
        "print(f\"Embedding dimension: {emb_train_full.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate embedding-based classifier -> WELFake\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Embedding-based Classifier - Full Train -> WELFake\")\n",
        "print(\"=\"*60)\n",
        "model_emb = train_embedding_classifier(emb_train_full, y_train_full)\n",
        "results_emb_welfake = evaluate(model_emb, emb_test_welfake_emb, y_test_welfake,\n",
        "                              model_name=\"Embedding Classifier (WELFake)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fake-only test: false negatives / fake recall (embeddings)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Fake-only test set (all fake) - Embedding model\")\n",
        "print(\"=\"*60)\n",
        "emb_preds_fake = model_emb.predict(emb_test_fake_emb)\n",
        "total_fake = len(emb_preds_fake)\n",
        "fn_fake = np.sum(emb_preds_fake == 0)\n",
        "recall_fake_emb = 1 - fn_fake / total_fake if total_fake else float('nan')\n",
        "print(f\"Embedding model: total={total_fake}, false_negatives={fn_fake}, fake_recall={recall_fake_emb:.4f}\")\n",
        "fake_only_emb = {\"total\": int(total_fake), \"false_negatives\": int(fn_fake), \"fake_recall\": float(recall_fake_emb)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Results Summary\n",
        "\n",
        "Compare models on WELFake (mixed labeled) and report fake-only recall on `data/test/fake.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile results summary\n",
        "print(\"=\"*60)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results_summary = pd.DataFrame({\n",
        "    'Model': [\n",
        "        'Logistic Regression (TF-IDF)',\n",
        "        'Linear SVM (TF-IDF)',\n",
        "        'Embedding Classifier',\n",
        "    ],\n",
        "    'WELFake - Macro-F1': [\n",
        "        results_lr_welfake['f1_macro'],\n",
        "        results_svm_welfake['f1_macro'],\n",
        "        results_emb_welfake['f1_macro'] if 'results_emb_welfake' in locals() else None,\n",
        "    ],\n",
        "    'WELFake - ROC-AUC': [\n",
        "        results_lr_welfake['roc_auc'],\n",
        "        results_svm_welfake['roc_auc'],\n",
        "        results_emb_welfake['roc_auc'] if 'results_emb_welfake' in locals() else None,\n",
        "    ],\n",
        "    'WELFake - PR-AUC': [\n",
        "        results_lr_welfake['pr_auc'],\n",
        "        results_svm_welfake['pr_auc'],\n",
        "        results_emb_welfake['pr_auc'] if 'results_emb_welfake' in locals() else None,\n",
        "    ],\n",
        "    'Fake-only recall': [\n",
        "        fake_only_lr['fake_recall'],\n",
        "        fake_only_svm['fake_recall'],\n",
        "        fake_only_emb['fake_recall'] if 'fake_only_emb' in locals() else None,\n",
        "    ],\n",
        "})\n",
        "\n",
        "print(\"\\nModel Performance Comparison:\")\n",
        "print(results_summary.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Cross-Dataset Transfer Evaluation\n",
        "\n",
        "Handled above by running the ISOT-trained models on WELFake (zero-shot, no fine-tuning). No additional code needed here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(Reference) Cross-dataset transfer summary: see WELFake results above; no further actions here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Conclusions and Discussion\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "1. **Model Performance**: Compare baseline (TF-IDF) vs. advanced (embeddings, transformers) approaches\n",
        "2. **Robustness**: Assess performance drop from random splits to topic-holdout splits\n",
        "3. **Generalization**: Evaluate cross-dataset transfer performance on WELFake\n",
        "\n",
        "### Interpretation:\n",
        "\n",
        "- **Macro-F1** serves as the primary metric to balance performance across classes\n",
        "- **Topic-holdout splits** reveal whether models rely on topic-specific shortcuts\n",
        "- **Cross-dataset evaluation** tests real-world applicability beyond ISOT\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Fine-tune hyperparameters for optimal performance\n",
        "- Analyze failure cases and model interpretability\n",
        "- Expand cross-dataset evaluation to additional external datasets\n",
        "- Add transformer model evaluation (requires additional setup)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
