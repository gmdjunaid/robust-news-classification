{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust News Classification: Main Experiments\n",
    "\n",
    "This notebook ties together all components of the robust news classification project for the current flow (train on full ISOT, test on held-out files).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Data Loading & Preprocessing**: Load ISOT training data and clean text\n",
    "2. **Training**: Train on the full ISOT training set (Fake + True)\n",
    "3. **Baseline & Advanced Models**: TF-IDF + LogReg/SVM; optional sentence-embedding classifier\n",
    "4. **Evaluation**:\n",
    "   - Fake-only test (`data/test/fake.csv`): false negatives / fake recall\n",
    "   - Mixed labeled test (`data/test/WELFake_Dataset_sample_1000.csv`): Macro-F1, ROC-AUC, PR-AUC, confusion matrix\n",
    "5. **Cross-Dataset Transfer**: WELFake evaluation covers the external test without fine-tuning\n",
    "\n",
    "## Project Goals\n",
    "\n",
    "- Train on full labeled ISOT data (text only)\n",
    "- Check false negatives on fake-only held-out data\n",
    "- Measure balanced metrics on a mixed external set (WELFake)\n",
    "- Compare TF-IDF baselines with optional embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import all necessary modules from the `src/` directory and configure settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "Project root: /Users/reuben/robust-news-classification/robust-news-classification\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path for imports\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "# Import using importlib for files with numeric prefixes\n",
    "import importlib.util\n",
    "\n",
    "# Import preprocessing utilities\n",
    "spec = importlib.util.spec_from_file_location(\"preprocessing\", project_root / \"src\" / \"01_preprocessing.py\")\n",
    "preprocessing = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(preprocessing)\n",
    "sys.modules[\"preprocessing\"] = preprocessing\n",
    "from preprocessing import load_isot, apply_cleaning, clean_text\n",
    "\n",
    "# Import baseline models\n",
    "spec = importlib.util.spec_from_file_location(\"baseline_models\", project_root / \"src\" / \"03_baseline_models.py\")\n",
    "baseline_models = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(baseline_models)\n",
    "sys.modules[\"baseline_models\"] = baseline_models\n",
    "from baseline_models import build_tfidf, train_logreg, train_svm\n",
    "\n",
    "# Import evaluation\n",
    "spec = importlib.util.spec_from_file_location(\"baseline_eval\", project_root / \"src\" / \"04_baseline_eval.py\")\n",
    "baseline_eval = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(baseline_eval)\n",
    "sys.modules[\"baseline_eval\"] = baseline_eval\n",
    "from baseline_eval import evaluate\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "Load the ISOT dataset (Fake and True news) and apply text cleaning to remove noise and standardize formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ISOT dataset...\n",
      "Loading fake news from ../data/training/Fake.csv...\n",
      "Loading real news from ../data/training/True.csv...\n",
      "Loaded 23481 fake articles and 21417 real articles\n",
      "Total: 44898 articles\n",
      "\n",
      "Dataset shape: (44898, 6)\n",
      "Columns: ['title', 'text', 'subject', 'date', 'label', 'source_file']\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "1    23481\n",
      "0    21417\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Subject distribution:\n",
      "subject\n",
      "politicsNews       11272\n",
      "worldnews          10145\n",
      "News                9050\n",
      "politics            6841\n",
      "left-news           4459\n",
      "Government News     1570\n",
      "US_News              783\n",
      "Middle-east          778\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "Applying text cleaning...\n",
      "Applied text cleaning to column 'text'\n",
      "Created new column 'text_cleaned' with cleaned text\n",
      "Applied text cleaning to column 'title'\n",
      "Created new column 'title_cleaned' with cleaned text\n",
      "\n",
      "Sample cleaned text (first 200 chars):\n",
      "Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and the very dishonest fake news media. The former reali\n"
     ]
    }
   ],
   "source": [
    "# Load ISOT dataset\n",
    "print(\"Loading ISOT dataset...\")\n",
    "df = load_isot(\n",
    "    fake_path='../data/training/Fake.csv',\n",
    "    real_path='../data/training/True.csv'\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nSubject distribution:\")\n",
    "print(df['subject'].value_counts())\n",
    "\n",
    "# Apply text cleaning\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Applying text cleaning...\")\n",
    "df = apply_cleaning(df, text_column='text')\n",
    "df = apply_cleaning(df, text_column='title')\n",
    "\n",
    "print(f\"\\nSample cleaned text (first 200 chars):\")\n",
    "print(df['text_cleaned'].iloc[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Test Setup\n",
    "\n",
    "Train on the full ISOT training set (Fake + True). Evaluate on two held-out files:\n",
    "- `data/test/fake.csv` (all fake) to measure false negatives (fake recall)\n",
    "- `data/test/WELFake_Dataset_sample_1000.csv` (mixed, labeled) for full metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Preparing train and held-out test sets\n",
      "============================================================\n",
      "\n",
      "Loading fake-only test set (all fake)...\n",
      "\n",
      "Loading WELFake test set (mixed labeled)...\n",
      "Train size: 44898\n",
      "Fake-only test size: 12999\n",
      "WELFake test size: 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Preparing train and held-out test sets\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train on full ISOT training set\n",
    "X_train_full = df['text_cleaned'].tolist()\n",
    "y_train_full = df['label'].values\n",
    "\n",
    "# Fake-only test set: measure false negatives / fake recall\n",
    "print(\"\\nLoading fake-only test set (all fake)...\")\n",
    "df_fake_test = pd.read_csv('../data/test/fake.csv')\n",
    "df_fake_test['text_cleaned'] = df_fake_test['text'].apply(clean_text)\n",
    "X_test_fake = df_fake_test['text_cleaned'].tolist()\n",
    "y_test_fake = np.ones(len(df_fake_test), dtype=int)\n",
    "\n",
    "# WELFake mixed test set: full metrics\n",
    "print(\"\\nLoading WELFake test set (mixed labeled)...\")\n",
    "df_welfake = pd.read_csv('../data/test/WELFake_Dataset_sample_1000.csv')\n",
    "# Map WELFake labels (source: 0=fake, 1=real) into project convention (1=fake, 0=real)\n",
    "df_welfake['label'] = df_welfake['label'].map({0: 1, 1: 0})\n",
    "df_welfake['text_cleaned'] = df_welfake['text'].apply(clean_text)\n",
    "X_test_welfake = df_welfake['text_cleaned'].tolist()\n",
    "y_test_welfake = df_welfake['label'].values\n",
    "\n",
    "print(f\"Train size: {len(X_train_full)}\")\n",
    "print(f\"Fake-only test size: {len(X_test_fake)}\")\n",
    "print(f\"WELFake test size: {len(X_test_welfake)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Models: TF-IDF + Linear Classifiers\n",
    "\n",
    "Train interpretable baseline models using TF-IDF features with Logistic Regression and Linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASELINE MODELS: TF-IDF + Linear Classifiers\n",
      "============================================================\n",
      "Created TF-IDF vectorizer with:\n",
      "  max_features: 5000\n",
      "  min_df: 2\n",
      "  max_df: 0.95\n",
      "  ngram_range: (1, 2)\n",
      "  stop_words: english\n",
      "\n",
      "TF-IDF feature matrix shape: (44898, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Build TF-IDF vectorizer\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE MODELS: TF-IDF + Linear Classifiers\")\n",
    "print(\"=\"*60)\n",
    "vectorizer = build_tfidf(max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "# Transform text data (fit once on full training set)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_full)\n",
    "X_test_fake_tfidf = vectorizer.transform(X_test_fake)\n",
    "X_test_welfake_tfidf = vectorizer.transform(X_test_welfake)\n",
    "\n",
    "print(f\"\\nTF-IDF feature matrix shape: {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Logistic Regression - Full Train -> WELFake\n",
      "============================================================\n",
      "Training Logistic Regression classifier...\n",
      "  Training samples: 44898\n",
      "  C (regularization): 1.0\n",
      "  max_iter: 1000\n",
      "  solver: lbfgs\n",
      "Training completed.\n",
      "  Training accuracy: 0.9925\n",
      "\n",
      "============================================================\n",
      "Evaluating Logistic Regression (WELFake)\n",
      "============================================================\n",
      "Test set size: 1000 samples\n",
      "Class distribution: {np.int64(0): np.int64(534), np.int64(1): np.int64(466)}\n",
      "\n",
      "Metrics                   Value          \n",
      "----------------------------------------\n",
      "Accuracy                  0.1550\n",
      "Precision (macro)         0.1204\n",
      "Recall (macro)            0.1654\n",
      "F1-score (macro)          0.1379  <-- PRIMARY METRIC\n",
      "F1-score (weighted)       0.1296\n",
      "ROC-AUC                   0.0953  <-- Secondary metric\n",
      "PR-AUC (Avg Precision)    0.2914  <-- Secondary metric\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predicted Real  Predicted Fake \n",
      "Actual Real     7               527            \n",
      "Actual Fake     318             148            \n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.02      0.01      0.02       534\n",
      "        Fake       0.22      0.32      0.26       466\n",
      "\n",
      "    accuracy                           0.15      1000\n",
      "   macro avg       0.12      0.17      0.14      1000\n",
      "weighted avg       0.11      0.15      0.13      1000\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Logistic Regression on full train -> WELFake\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Logistic Regression - Full Train -> WELFake\")\n",
    "print(\"=\"*60)\n",
    "model_lr = train_logreg(X_train_tfidf, y_train_full)\n",
    "results_lr_welfake = evaluate(model_lr, X_test_welfake_tfidf, y_test_welfake, \n",
    "                             model_name=\"Logistic Regression (WELFake)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Linear SVM - Full Train -> WELFake\n",
      "============================================================\n",
      "Training Linear SVM classifier...\n",
      "  Training samples: 44898\n",
      "  C (regularization): 1.0\n",
      "  max_iter: 1000\n",
      "  dual: False\n",
      "Training completed.\n",
      "  Training accuracy: 0.9990\n",
      "\n",
      "============================================================\n",
      "Evaluating Linear SVM (WELFake)\n",
      "============================================================\n",
      "Test set size: 1000 samples\n",
      "Class distribution: {np.int64(0): np.int64(534), np.int64(1): np.int64(466)}\n",
      "\n",
      "Metrics                   Value          \n",
      "----------------------------------------\n",
      "Accuracy                  0.1650\n",
      "Precision (macro)         0.1253\n",
      "Recall (macro)            0.1762\n",
      "F1-score (macro)          0.1450  <-- PRIMARY METRIC\n",
      "F1-score (weighted)       0.1361\n",
      "ROC-AUC                   0.0947  <-- Secondary metric\n",
      "PR-AUC (Avg Precision)    0.2913  <-- Secondary metric\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predicted Real  Predicted Fake \n",
      "Actual Real     6               528            \n",
      "Actual Fake     307             159            \n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.02      0.01      0.01       534\n",
      "        Fake       0.23      0.34      0.28       466\n",
      "\n",
      "    accuracy                           0.17      1000\n",
      "   macro avg       0.13      0.18      0.14      1000\n",
      "weighted avg       0.12      0.17      0.14      1000\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Linear SVM on full train -> WELFake\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Linear SVM - Full Train -> WELFake\")\n",
    "print(\"=\"*60)\n",
    "model_svm = train_svm(X_train_tfidf, y_train_full)\n",
    "results_svm_welfake = evaluate(model_svm, X_test_welfake_tfidf, y_test_welfake,\n",
    "                              model_name=\"Linear SVM (WELFake)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Fake-only test set (all fake) - TF-IDF models\n",
      "============================================================\n",
      "LogReg (fake-only): total=12999, false_negatives=826, fake_recall=0.9365\n",
      "Linear SVM (fake-only): total=12999, false_negatives=685, fake_recall=0.9473\n"
     ]
    }
   ],
   "source": [
    "# Fake-only test: false negatives / fake recall (TF-IDF models)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Fake-only test set (all fake) - TF-IDF models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def fake_only_report(model, X_fake, model_name):\n",
    "    preds = model.predict(X_fake)\n",
    "    total = len(preds)\n",
    "    fn = np.sum(preds == 0)\n",
    "    recall_fake = 1 - fn / total if total else float('nan')\n",
    "    print(f\"{model_name}: total={total}, false_negatives={fn}, fake_recall={recall_fake:.4f}\")\n",
    "    return {\"total\": int(total), \"false_negatives\": int(fn), \"fake_recall\": float(recall_fake)}\n",
    "\n",
    "fake_only_lr = fake_only_report(model_lr, X_test_fake_tfidf, \"LogReg (fake-only)\")\n",
    "fake_only_svm = fake_only_report(model_svm, X_test_fake_tfidf, \"Linear SVM (fake-only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Placeholder removed after refactor; no additional code needed here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Models: Sentence Embeddings\n",
    "\n",
    "Train models using sentence embeddings as features, providing richer semantic representations than TF-IDF.\n",
    "\n",
    "**Note**: This section requires the `sentence-transformers` library. Uncomment and run if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ADVANCED MODELS: Sentence Embeddings\n",
      "============================================================\n",
      "Loading sentence-embedding model: all-MiniLM-L6-v2\n",
      "Embedding model loaded successfully.\n",
      "\n",
      "Computing embeddings for full train and test sets...\n",
      "Encoding 44898 texts into embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a03aa755da4df698832b9925316368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (44898, 384)\n",
      "Encoding 12999 texts into embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6ed09f643a4bf4aeb1204091019905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (12999, 384)\n",
      "Encoding 1000 texts into embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d3f7f2a7a74cc3a08a5ab96d9124df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1000, 384)\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Import embedding utilities\n",
    "spec = importlib.util.spec_from_file_location(\"embeddings_model\", project_root / \"src\" / \"05_embeddings_model.py\")\n",
    "embeddings_model = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(embeddings_model)\n",
    "sys.modules[\"embeddings_model\"] = embeddings_model\n",
    "from embeddings_model import build_embeddings, embed_text, train_embedding_classifier\n",
    "\n",
    "# Build sentence embedding model\n",
    "print(\"=\"*60)\n",
    "print(\"ADVANCED MODELS: Sentence Embeddings\")\n",
    "print(\"=\"*60)\n",
    "embedder = build_embeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Compute embeddings for full train and tests\n",
    "print(\"\\nComputing embeddings for full train and test sets...\")\n",
    "emb_train_full = embed_text(embedder, X_train_full)\n",
    "emb_test_fake_emb = embed_text(embedder, X_test_fake)\n",
    "emb_test_welfake_emb = embed_text(embedder, X_test_welfake)\n",
    "\n",
    "print(f\"Embedding dimension: {emb_train_full.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Embedding-based Classifier - Full Train -> WELFake\n",
      "============================================================\n",
      "Training Logistic Regression classifier on embeddings...\n",
      "  Training samples: 44898\n",
      "  Embedding dimension: 384\n",
      "  C (regularization): 1.0\n",
      "  max_iter: 5000\n",
      "Training completed.\n",
      "  Training accuracy on embeddings: 0.9617\n",
      "\n",
      "============================================================\n",
      "Evaluating Embedding Classifier (WELFake)\n",
      "============================================================\n",
      "Test set size: 1000 samples\n",
      "Class distribution: {np.int64(0): np.int64(534), np.int64(1): np.int64(466)}\n",
      "\n",
      "Metrics                   Value          \n",
      "----------------------------------------\n",
      "Accuracy                  0.1880\n",
      "Precision (macro)         0.1800\n",
      "Recall (macro)            0.1941\n",
      "F1-score (macro)          0.1833  <-- PRIMARY METRIC\n",
      "F1-score (weighted)       0.1791\n",
      "ROC-AUC                   0.1087  <-- Secondary metric\n",
      "PR-AUC (Avg Precision)    0.2940  <-- Secondary metric\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predicted Real  Predicted Fake \n",
      "Actual Real     56              478            \n",
      "Actual Fake     334             132            \n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.14      0.10      0.12       534\n",
      "        Fake       0.22      0.28      0.25       466\n",
      "\n",
      "    accuracy                           0.19      1000\n",
      "   macro avg       0.18      0.19      0.18      1000\n",
      "weighted avg       0.18      0.19      0.18      1000\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate embedding-based classifier -> WELFake\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Embedding-based Classifier - Full Train -> WELFake\")\n",
    "print(\"=\"*60)\n",
    "model_emb = train_embedding_classifier(emb_train_full, y_train_full)\n",
    "results_emb_welfake = evaluate(model_emb, emb_test_welfake_emb, y_test_welfake,\n",
    "                              model_name=\"Embedding Classifier (WELFake)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Fake-only test set (all fake) - Embedding model\n",
      "============================================================\n",
      "Embedding model: total=12999, false_negatives=3235, fake_recall=0.7511\n"
     ]
    }
   ],
   "source": [
    "# Fake-only test: false negatives / fake recall (embeddings)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Fake-only test set (all fake) - Embedding model\")\n",
    "print(\"=\"*60)\n",
    "emb_preds_fake = model_emb.predict(emb_test_fake_emb)\n",
    "total_fake = len(emb_preds_fake)\n",
    "fn_fake = np.sum(emb_preds_fake == 0)\n",
    "recall_fake_emb = 1 - fn_fake / total_fake if total_fake else float('nan')\n",
    "print(f\"Embedding model: total={total_fake}, false_negatives={fn_fake}, fake_recall={recall_fake_emb:.4f}\")\n",
    "fake_only_emb = {\"total\": int(total_fake), \"false_negatives\": int(fn_fake), \"fake_recall\": float(recall_fake_emb)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Summary\n",
    "\n",
    "Compare models on WELFake (mixed labeled) and report fake-only recall on `data/test/fake.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RESULTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Model Performance Comparison:\n",
      "                       Model  WELFake - Macro-F1  WELFake - ROC-AUC  WELFake - PR-AUC  Fake-only recall\n",
      "Logistic Regression (TF-IDF)            0.137860           0.095293          0.291413          0.936457\n",
      "         Linear SVM (TF-IDF)            0.144985           0.094742          0.291325          0.947304\n",
      "        Embedding Classifier            0.183283           0.108731          0.294032          0.751135\n"
     ]
    }
   ],
   "source": [
    "# Compile results summary\n",
    "print(\"=\"*60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_summary = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Logistic Regression (TF-IDF)',\n",
    "        'Linear SVM (TF-IDF)',\n",
    "        'Embedding Classifier',\n",
    "    ],\n",
    "    'WELFake - Macro-F1': [\n",
    "        results_lr_welfake['f1_macro'],\n",
    "        results_svm_welfake['f1_macro'],\n",
    "        results_emb_welfake['f1_macro'] if 'results_emb_welfake' in locals() else None,\n",
    "    ],\n",
    "    'WELFake - ROC-AUC': [\n",
    "        results_lr_welfake['roc_auc'],\n",
    "        results_svm_welfake['roc_auc'],\n",
    "        results_emb_welfake['roc_auc'] if 'results_emb_welfake' in locals() else None,\n",
    "    ],\n",
    "    'WELFake - PR-AUC': [\n",
    "        results_lr_welfake['pr_auc'],\n",
    "        results_svm_welfake['pr_auc'],\n",
    "        results_emb_welfake['pr_auc'] if 'results_emb_welfake' in locals() else None,\n",
    "    ],\n",
    "    'Fake-only recall': [\n",
    "        fake_only_lr['fake_recall'],\n",
    "        fake_only_svm['fake_recall'],\n",
    "        fake_only_emb['fake_recall'] if 'fake_only_emb' in locals() else None,\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(results_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Dataset Transfer Evaluation\n",
    "\n",
    "Handled above by running the ISOT-trained models on WELFake (zero-shot, no fine-tuning). No additional code needed here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Reference) Cross-dataset transfer summary: see WELFake results above; no further actions here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions and Discussion\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Model Performance**: Compare baseline (TF-IDF) vs. advanced (embeddings, transformers) approaches\n",
    "2. **Robustness**: Assess performance drop from random splits to topic-holdout splits\n",
    "3. **Generalization**: Evaluate cross-dataset transfer performance on WELFake\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- **Macro-F1** serves as the primary metric to balance performance across classes\n",
    "- **Topic-holdout splits** reveal whether models rely on topic-specific shortcuts\n",
    "- **Cross-dataset evaluation** tests real-world applicability beyond ISOT\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Fine-tune hyperparameters for optimal performance\n",
    "- Analyze failure cases and model interpretability\n",
    "- Expand cross-dataset evaluation to additional external datasets\n",
    "- Add transformer model evaluation (requires additional setup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
